{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "eKznLy7yStES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary libraries and create the CartPole environment."
      ],
      "metadata": {
        "id": "vgCfy-_oxgXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the gym environment\n",
        "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
        "# Display the number of actions the agent can take\n",
        "print(env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpVGKX1vxE4G",
        "outputId": "00f2f123-43b2-4127-ce9f-5aa269d7df2b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Q-Learning"
      ],
      "metadata": {
        "id": "1YNW484kxreB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this task, I will implement the Q-Learning algorithm, since it is one of the simplest RL algorithms and the CartPole project is an appropriately simple task. It is a model-free RL architecture, so the agent actually learns policies directly. \"Q-Learning uses previously learned 'states' which have been explored to consider future moves and stores this information in a “Q-Table.” For every action taken from a state, the policy table, Q table, has to include a positive or negative reward\" (Fakhry, 2020 par. 3).\n",
        "\n",
        "First, some hyperparameters can be defined (these may be tweaked later as necessary). It is also necessary to define a function for getting discrete states for use with the RL algorithm:"
      ],
      "metadata": {
        "id": "nVlg3-_21Nn_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "NUVeCUc4SdQK"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "ALPHA = 0.1       # learning rate\n",
        "GAMMA = 0.99      # discount factor\n",
        "EPSILON = 1.0     # exploration rate\n",
        "EPSILON_DECAY = 0.995\n",
        "MIN_EPSILON = 0.01\n",
        "EPISODES = 500   # number of episodes to train\n",
        "\n",
        "# Discretize the continuous state space\n",
        "def discretize_state(state, bins):\n",
        "    discretized_state = []\n",
        "    for i in range(len(state)):\n",
        "        # Set upper and lower bounds to prevent overflow\n",
        "        high = np.minimum(env.observation_space.high[i], 1e5)\n",
        "        low = np.maximum(env.observation_space.low[i], -1e5)\n",
        "\n",
        "        # Scale the state variable to [0, bins-1]\n",
        "        scaling = (state[i] - low) / (high - low)\n",
        "        new_state = int(np.round(scaling * (bins - 1)))\n",
        "        new_state = np.clip(new_state, 0, bins - 1)\n",
        "        discretized_state.append(new_state)\n",
        "    return tuple(discretized_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the number of bins can be explicitly defined for the state variables. This is important for the Q-table, where every entry is an estimate of the total reward that corresponds to a combination of a state and action."
      ],
      "metadata": {
        "id": "0_wixaB93hc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Q-table\n",
        "n_bins = 20\n",
        "n_states = (n_bins, n_bins, n_bins, n_bins)\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(n_states + (env.action_space.n,)))\n",
        "\n",
        "episode_rewards = [] # Init episode rewards"
      ],
      "metadata": {
        "id": "J8nxKt923ufZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "S5jgUHLI7uD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training look is primarily seeking to update the Q-values based on reward received and the potential future rewards. When the epsilon decreases throughout the episodes, it indicates a shift toward 'exploitation', meaning the agent benefits from choosing the best action based on the Q-values rather than selecting randomly."
      ],
      "metadata": {
        "id": "3NlIg-kXoMUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-learning algorithm\n",
        "for episode in range(EPISODES):\n",
        "    current_state = discretize_state(env.reset(), n_bins)\n",
        "    total_reward = 0 # Accumulate total reward per episode\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.random() < EPSILON:\n",
        "            action = env.action_space.sample()  # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[current_state])  # Exploit learned values\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_state = discretize_state(next_state, n_bins)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Update Q-table\n",
        "        old_value = q_table[current_state + (action,)]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "\n",
        "        # Q-learning formula\n",
        "        new_value = (1 - ALPHA) * old_value + ALPHA * (reward + GAMMA * next_max)\n",
        "        q_table[current_state + (action,)] = new_value\n",
        "\n",
        "        current_state = next_state\n",
        "\n",
        "    # Appent total reward of the episode\n",
        "    episode_rewards.append(total_reward)\n",
        "\n",
        "    # Decrement epsilon\n",
        "    EPSILON = max(MIN_EPSILON, EPSILON * EPSILON_DECAY)\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        average_reward = sum(episode_rewards[-100:]) / 100\n",
        "        print(f\"Episode: {episode}, Average Reward: {average_reward}, Epsilon: {EPSILON}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3BKwjkz1Mey",
        "outputId": "4f8b27b2-790b-4a46-fdd1-fb21541f6cac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Average Reward: 0.34, Epsilon: 0.995\n",
            "Episode: 100, Average Reward: 16.84, Epsilon: 0.6027415843082742\n",
            "Episode: 200, Average Reward: 12.82, Epsilon: 0.36512303261753626\n",
            "Episode: 300, Average Reward: 12.49, Epsilon: 0.2211807388415433\n",
            "Episode: 400, Average Reward: 19.04, Epsilon: 0.13398475271138335\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The epsilon is indeed decreasing, exemplifying the shift from random exploration to informed exploitation. This also serves to stablize the Q-table to coverge toward an optimal decision-making policy. Furthermore, logging the average reward can track the performance of the agent over time. In my case, although the reward decreased for a few iterations, it overall increased by the final episode."
      ],
      "metadata": {
        "id": "1HCvO7V0AuOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "tTfWhD1aSv4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent's performance can best be visualized by rendering the environment. The following code visualizes an episode run."
      ],
      "metadata": {
        "id": "C4sDdZc9vpa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To visualize one episode run\n",
        "state = env.reset()\n",
        "for _ in range(100):\n",
        "    env.render()\n",
        "    action = np.argmax(q_table[discretize_state(state, n_bins)])\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Prz6y75QwZ3v"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used just 5 episodes in order to reduce the training time, although I realize more episodes would lead to better performance. NOTE: I realized that visually rendering the environment does not translate well to Google Colab. Ordinarily, a separate window would appear and display the environment. So, the code does not behave as expected. Overall, I enjoyed the change of scenery that this form of machine learning provided. It is definitely much different from the other techniques we have covered."
      ],
      "metadata": {
        "id": "DcMLFz890REu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "VYkC6nzzSyvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fakhry, A. (2020, November 13). *Using Q-learning for OpenAI’s Cartpole-V1*. Medium. https://medium.com/swlh/using-q-learning-for-openais-cartpole-v1-4a216ef237df\n",
        "\n",
        "\n",
        "Géron, A. (2023a). *Chapter 18 – Reinforcement Learning Code*. Google Colab. https://colab.research.google.com/github/ageron/handson-ml3/blob/main/18_reinforcement_learning.ipynb#scrollTo=rF0zo2-xyK8_\n",
        "\n",
        "\n",
        "Géron, A. (2023b). *Hands-On Machine Learning With Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems*. O’Reilly Media."
      ],
      "metadata": {
        "id": "WxFnEZJ06ZIM"
      }
    }
  ]
}
