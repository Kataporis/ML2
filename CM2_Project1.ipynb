{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Changes in This Version"
      ],
      "metadata": {
        "id": "525ArfutqEdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Gradually increased the number of hidden layers starting with 3.\n",
        "2. Added droput and batch normalization after this."
      ],
      "metadata": {
        "id": "oeMXJ74vqHV2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NA7B1IEp0PH"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpQee9KWqtZ2"
      },
      "source": [
        "Ensure that Python version 3.7 or above is installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5RExu4uq27y"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zszn9685wZJ1"
      },
      "source": [
        "Import TensorFlow ≥ 2.8 * math:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5en5voMwesD"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDNxVuhPzIHj"
      },
      "source": [
        "# Building the Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SndaAk7SFFA3"
      },
      "source": [
        "I initially built the neural network with 3 hidden layers of 50 neurons each. Next, I added another hidden layer (4 total). I then added one more layer to see if there would be any effect. Furthermore, I employed the swish activation function and the he_normal kernel initializer. In order to ensure the network self-normalizes, however, I standardized the input features, changed the activation function to selu, and switched to LeCun normal initialization. Finally, I added batch normalization layers to see how they would affect things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow07_ID3_4AO"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(3):\n",
        "    model.add(tf.keras.layers.Dense(50, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
        "    model.add(tf.keras.layers.BatchNormalization())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSax1wt86uyT"
      },
      "source": [
        "After some lackluster results, I decided to add some regularization with alpha dropout, as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKCymsXV680W"
      },
      "outputs": [],
      "source": [
        "model.add(tf.keras.layers.AlphaDropout(rate = 0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69Kh8WFVXuD2"
      },
      "source": [
        "# Training the Network on the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwVN7JOyG8_f"
      },
      "source": [
        "Next, the network was trained on the CIFAR10 dataset using Nadam optimization. I chose to train the network on the CIFAR10 dataset because it is one of the most common datasets for learning machine learning and is already built into Keras. Furthermore, the author of our course textbook Géron provided their approach to developing a model for this dataset in a Google Colab notebook. I frequently referenced this notebook and used it as a baseline for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQbdSHPnhtwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09278007-ac98-4edc-eb63-365db5f3233e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "cifar10 = tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y_P3_F3iKUh"
      },
      "source": [
        "This dataset is composed of 60,000 32 x 32-pixel color images. 50,000 of these will be set aside for the training set, and the rest (10,000) will make up the testing set. There are 10 classes in this dataset, so the output layer should have 10 neurons with the softmax activation function. The softmax function scales numbers into probabilities. I added the output layer to the model as such:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg8z2qhbjGWG"
      },
      "outputs": [],
      "source": [
        "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ANKrHNwnvh9"
      },
      "source": [
        "I compiled the model and specified the stochastic gradient descent optimizer (I initially employed Nadam), sparse categorical crossentropy loss function, and accuracy metric to evaluate the the performance of the model. By compiling the model, \"the backend automatically chooses the best way to represent the network for training and making predictions to run on [the] hardware\" (Brownlee, 2022)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cca0CDb4o63_"
      },
      "outputs": [],
      "source": [
        "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
        "              optimizer = tf.keras.optimizers.SGD(),\n",
        "              metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmNoDEndqH2c"
      },
      "source": [
        "Note: The learning rate associated with the optimizer can be manually tuned by comparing the learning curves for different rates for a set number of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHfLSpz9rK6N"
      },
      "source": [
        "# Fitting the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9F3EvymuQ5c"
      },
      "source": [
        "In order to the train (fit) the model, it is first necessary to split the data into subsets. First, I split the data into only a training and testing subset, but I quickyl realized I would also need a validation set to best evaluate the model's performance. The validation set is composed of 5,000 images of the original training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctPpJ_Z8v8vQ"
      },
      "outputs": [],
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = cifar10\n",
        "\n",
        "X_train = X_train_full[5000:]\n",
        "y_train = y_train_full[5000:]\n",
        "X_valid = X_train_full[:5000]\n",
        "y_valid = y_train_full[:5000]\n",
        "\n",
        "X_means = X_train.mean(axis = 0)\n",
        "X_stds = X_train.std(axis = 0)\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled = (X_test - X_means) / X_stds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQyxzpLxFEfT"
      },
      "source": [
        "I decided to add 1cycle scheduling after hearing about how powerful it can be for this particular dataset. I utilized (Géron 2023)'s OneCycleScheduler custom callback class, find_learning_rate function, and ExponentialLearningRate callback class from the Chapter 11 Code for this purpose:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOkqPpVtLsxc"
      },
      "outputs": [],
      "source": [
        "K = tf.keras.backend\n",
        "# Updates the learning rate at the beginning of each batch. Increases learning rate linearly\n",
        "# during about the first half of training, then reduces it linearly back to initial\n",
        "# learning rate. Finally, it reduces it down to close to zero for the last part of\n",
        "# training.\n",
        "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, iterations, max_lr=1e-3, start_lr=None,\n",
        "                 last_iterations=None, last_lr=None):\n",
        "        self.iterations = iterations\n",
        "        self.max_lr = max_lr\n",
        "        self.start_lr = start_lr or max_lr / 10\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_lr = last_lr or self.start_lr / 1000\n",
        "        self.iteration = 0\n",
        "\n",
        "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
        "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
        "\n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            lr = self._interpolate(0, self.half_iteration, self.start_lr,\n",
        "                                   self.max_lr)\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
        "                                   self.max_lr, self.start_lr)\n",
        "        else:\n",
        "            lr = self._interpolate(2 * self.half_iteration, self.iterations,\n",
        "                                   self.start_lr, self.last_lr)\n",
        "        self.iteration += 1\n",
        "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
        "\n",
        "# Updates the learning rate during training at the end of each batch.\n",
        "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "        self.rates = []\n",
        "        self.losses = []\n",
        "\n",
        "# Trains the model using the ExponentialLearningRate callback and returns the\n",
        "# learning rates & batch losses.\n",
        "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=1e-4,\n",
        "                       max_rate=1):\n",
        "    init_weights = model.get_weights()\n",
        "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
        "    factor = (max_rate / min_rate) ** (1 / iterations)\n",
        "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
        "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
        "    exp_lr = ExponentialLearningRate(factor)\n",
        "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
        "                        callbacks=[exp_lr])\n",
        "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
        "    model.set_weights(init_weights)\n",
        "    return exp_lr.rates, exp_lr.losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGeL9TalFj_I",
        "outputId": "2734fc44-0625-46e1-bc08-ba4efc89679c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "352/352 [==============================] - 5s 7ms/step - loss: 2.6964 - accuracy: 0.1425\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "num_iterations = math.ceil(len(X_train_scaled) / batch_size) * num_epochs\n",
        "onecycle = OneCycleScheduler(num_iterations, max_lr = 0.05)\n",
        "# This finds the optimal max learning rate\n",
        "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1,\n",
        "                                   batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fc0ZHc5LtaR"
      },
      "source": [
        "Now, the model can be trained on the training set over 10 epochs\n",
        ":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UCTVq2ELxlL",
        "outputId": "972fa3e6-4e58-4386-ecda-9b622945f81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "352/352 [==============================] - 4s 10ms/step - loss: 2.0718 - accuracy: 0.2840 - val_loss: 1.7060 - val_accuracy: 0.4012\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 1.8114 - accuracy: 0.3637 - val_loss: 1.6299 - val_accuracy: 0.4230\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 1.6858 - accuracy: 0.4059 - val_loss: 1.5799 - val_accuracy: 0.4426\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 3s 8ms/step - loss: 1.6034 - accuracy: 0.4336 - val_loss: 1.5519 - val_accuracy: 0.4554\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 1.5407 - accuracy: 0.4549 - val_loss: 1.5379 - val_accuracy: 0.4574\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 1.4773 - accuracy: 0.4771 - val_loss: 1.4707 - val_accuracy: 0.4866\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 1.4315 - accuracy: 0.4941 - val_loss: 1.4562 - val_accuracy: 0.4834\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 1.3927 - accuracy: 0.5051 - val_loss: 1.4417 - val_accuracy: 0.4922\n",
            "Epoch 9/10\n",
            "298/352 [========================>.....] - ETA: 0s - loss: 1.3645 - accuracy: 0.5171"
          ]
        }
      ],
      "source": [
        "model.fit(X_train_scaled, y_train, epochs = num_epochs, batch_size = batch_size,\n",
        "          validation_data = (X_valid_scaled, y_valid),\n",
        "          callbacks = [onecycle])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "vNEzxy02hUiZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l5v5YLtOYHn"
      },
      "source": [
        "Finally, the model can be evaluated on the validation data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YsJuflpOe9L",
        "outputId": "eb8f5344-e0e9-4e39-9c10-c0254ca3d429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 0s 3ms/step - loss: 1.4237 - accuracy: 0.5086\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 3072)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 50)                153650    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " alpha_dropout (AlphaDropou  (None, 50)                0         \n",
            " t)                                                              \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 159260 (622.11 KB)\n",
            "Trainable params: 159260 (622.11 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.evaluate(X_valid_scaled, y_valid)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-3_W_O2Oow8"
      },
      "source": [
        "**Layers:**\n",
        "\n",
        "First, I started off with 3 hidden layers. I was shocked to find out that my accuracy (52%) actually increased when compared to 20 layers (48%).\n",
        "\n",
        "Second, I added another hidden layer (4 total), though it did not make much of a difference.\n",
        "\n",
        "Third, I added yet another hidden layer (5 total). Once again, it did made a negligible difference, so I decided to stick with the initial amount of hidden layers (3 total).\n",
        "\n",
        "**Dropout:**\n",
        "\n",
        "After some promising results, I decided to add an alpha dropout layer to see how things would change. The change in results was negligible, however.\n",
        "\n",
        "**Batch Normalization:**\n",
        "I added batch normalization, though I heard it does not work as well when used in combination with self-normalization. This conflicts with the selu activation function and alpha dropout, but I tried it out anyway. The change in results was once again negligible.\n",
        "\n",
        "I got to witness the importance of all of these small changes on the performance of the model over time, showcasing how advanced ML techniques have improved model performance over time. I also learned that sometimes less is more when it comes to the amount of layers and neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOz3Q_koSJxJ"
      },
      "source": [
        "# Works Cited (APA7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG0IUoQNTJMF"
      },
      "source": [
        "Brownlee, J. (2022, August 16). *Your first deep learning project in python with keras step-by-step.* MachineLearningMastery.com. https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
        "\n",
        "Géron, A. (2023). *Hands-on machine learning with scikit-learn, keras and tensorflow: Concepts, tools, and techniques to build Intelligent Systems* (3rd ed.). O’Reilly Media, Inc.\n",
        "\n",
        "Géron, A. (n.d.). Google Colab. *Chapter 11 – Training Deep Neural Networks*. https://colab.research.google.com/github/ageron/handson-ml3/blob/main/11_training_deep_neural_networks.ipynb\n",
        "\n",
        "Google. (n.d.). *Machine Learning Crash Course with TensorFlow APIs*. Google Machine Learning. https://developers.google.com/machine-learning/crash-course\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
