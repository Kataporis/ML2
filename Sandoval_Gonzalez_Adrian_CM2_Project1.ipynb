{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "4NA7B1IEp0PH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure that Python version 3.7 or above is installed:"
      ],
      "metadata": {
        "id": "VpQee9KWqtZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ],
      "metadata": {
        "id": "U5RExu4uq27y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import TensorFlow ≥ 2.8 * math:"
      ],
      "metadata": {
        "id": "Zszn9685wZJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ],
      "metadata": {
        "id": "X5en5voMwesD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Deep Neural Network"
      ],
      "metadata": {
        "id": "HDNxVuhPzIHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I initially built the neural network with 5 hidden layers of 50 neurons each, though I ended up doubling the number of layers twice until I had 20. Furthermore, I employed he swish activation function and the he_normal kernel initializer. In order ensure the network self-normalizes, however, I standardized the input features, changed the activation function to selu, and switched to LeCun normal initialization."
      ],
      "metadata": {
        "id": "SndaAk7SFFA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(tf.keras.layers.Dense(50,\n",
        "                                    activation=\"selu\",\n",
        "                                    kernel_initializer=\"lecun_normal\"))"
      ],
      "metadata": {
        "id": "Ow07_ID3_4AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After some lackluster results, I decided to add some regularization with alpha dropout, as well."
      ],
      "metadata": {
        "id": "QSax1wt86uyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.AlphaDropout(rate = 0.1))"
      ],
      "metadata": {
        "id": "RKCymsXV680W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Network on the Dataset"
      ],
      "metadata": {
        "id": "69Kh8WFVXuD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the network was trained on the CIFAR10 dataset using Nadam optimization. I chose to train the network on the CIFAR10 dataset because it is one of the most common datasets for learning machine learning and is already built into Keras. Furthermore, the author of our course textbook Géron provided their approach to developing a model for this dataset in a Google Colab notebook. I frequently referenced this notebook and used it as a baseline for comparison."
      ],
      "metadata": {
        "id": "PwVN7JOyG8_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = tf.keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "LQbdSHPnhtwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7f1b57-db00-402c-847e-321cc178be30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 11s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is composed of 60,000 32 x 32-pixel color images. 50,000 of these will be set aside for the training set, and the rest (10,000) will make up the testing set. There are 10 classes in this dataset, so the output layer should have 10 neurons with the softmax activation function. The softmax function scales numbers into probabilities. I added the output layer to the model as such:"
      ],
      "metadata": {
        "id": "0Y_P3_F3iKUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))"
      ],
      "metadata": {
        "id": "gg8z2qhbjGWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I compiled the model and specified the stochastic gradient descent optimizer (I initially employed Nadam), sparse categorical crossentropy loss function, and accuracy metric to evaluate the the performance of the model. By compiling the model, \"the backend automatically chooses the best way to represent the network for training and making predictions to run on [the] hardware\" (Brownlee, 2022)."
      ],
      "metadata": {
        "id": "_ANKrHNwnvh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
        "              optimizer = tf.keras.optimizers.SGD(),\n",
        "              metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "Cca0CDb4o63_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The learning rate associated with the optimizer can be manually tuned by comparing the learning curves for different rates for a set number of epochs."
      ],
      "metadata": {
        "id": "FmNoDEndqH2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting the Model"
      ],
      "metadata": {
        "id": "nHfLSpz9rK6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to the train (fit) the model, it is first necessary to split the data into subsets. First, I split the data into only a training and testing subset, but I quickyl realized I would also need a validation set to best evaluate the model's performance. The validation set is composed of 5,000 images of the original training set."
      ],
      "metadata": {
        "id": "g9F3EvymuQ5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = cifar10\n",
        "\n",
        "X_train = X_train_full[5000:]\n",
        "y_train = y_train_full[5000:]\n",
        "X_valid = X_train_full[:5000]\n",
        "y_valid = y_train_full[:5000]\n",
        "\n",
        "X_means = X_train.mean(axis = 0)\n",
        "X_stds = X_train.std(axis = 0)\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled = (X_test - X_means) / X_stds"
      ],
      "metadata": {
        "id": "ctPpJ_Z8v8vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to add 1cycle scheduling after hearing about how powerful it can be for this particular dataset. I utilized (Géron 2023)'s OneCycleScheduler custom callback class, find_learning_rate function, and ExponentialLearningRate callback class from the Chapter 11 Code for this purpose:"
      ],
      "metadata": {
        "id": "BQyxzpLxFEfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = tf.keras.backend\n",
        "# Updates the learning rate at the beginning of each batch. Increases learning rate linearly\n",
        "# during about the first half of training, then reduces it linearly back to initial\n",
        "# learning rate. Finally, it reduces it down to close to zero for the last part of\n",
        "# training.\n",
        "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, iterations, max_lr=1e-3, start_lr=None,\n",
        "                 last_iterations=None, last_lr=None):\n",
        "        self.iterations = iterations\n",
        "        self.max_lr = max_lr\n",
        "        self.start_lr = start_lr or max_lr / 10\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_lr = last_lr or self.start_lr / 1000\n",
        "        self.iteration = 0\n",
        "\n",
        "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
        "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
        "\n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            lr = self._interpolate(0, self.half_iteration, self.start_lr,\n",
        "                                   self.max_lr)\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
        "                                   self.max_lr, self.start_lr)\n",
        "        else:\n",
        "            lr = self._interpolate(2 * self.half_iteration, self.iterations,\n",
        "                                   self.start_lr, self.last_lr)\n",
        "        self.iteration += 1\n",
        "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
        "\n",
        "# Updates the learning rate during training at the end of each batch.\n",
        "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "        self.rates = []\n",
        "        self.losses = []\n",
        "\n",
        "# Trains the model using the ExponentialLearningRate callback and returns the\n",
        "# learning rates & batch losses.\n",
        "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=1e-4,\n",
        "                       max_rate=1):\n",
        "    init_weights = model.get_weights()\n",
        "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
        "    factor = (max_rate / min_rate) ** (1 / iterations)\n",
        "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
        "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
        "    exp_lr = ExponentialLearningRate(factor)\n",
        "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
        "                        callbacks=[exp_lr])\n",
        "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
        "    model.set_weights(init_weights)\n",
        "    return exp_lr.rates, exp_lr.losses"
      ],
      "metadata": {
        "id": "TOkqPpVtLsxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "num_iterations = math.ceil(len(X_train_scaled) / batch_size) * num_epochs\n",
        "onecycle = OneCycleScheduler(num_iterations, max_lr = 0.05)\n",
        "# This finds the optimal max learning rate\n",
        "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1,\n",
        "                                   batch_size=batch_size)"
      ],
      "metadata": {
        "id": "qGeL9TalFj_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5476f11e-c600-42e2-a7e0-5aca7d4d8075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "352/352 [==============================] - 5s 10ms/step - loss: 2.6105 - accuracy: 0.1350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the model can be trained on the training set over 10 epochs\n",
        ":"
      ],
      "metadata": {
        "id": "7Fc0ZHc5LtaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_scaled, y_train, epochs = num_epochs, batch_size = batch_size,\n",
        "          validation_data = (X_valid_scaled, y_valid),\n",
        "          callbacks = [onecycle])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UCTVq2ELxlL",
        "outputId": "24fe2860-8837-4d6f-892c-cbec856cb432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 2.0972 - accuracy: 0.2610 - val_loss: 1.8498 - val_accuracy: 0.3422\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 4s 13ms/step - loss: 1.8164 - accuracy: 0.3500 - val_loss: 1.7207 - val_accuracy: 0.3784\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 1.6975 - accuracy: 0.3952 - val_loss: 1.6690 - val_accuracy: 0.4000\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.6288 - accuracy: 0.4193 - val_loss: 1.7541 - val_accuracy: 0.4004\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 4s 12ms/step - loss: 1.5780 - accuracy: 0.4408 - val_loss: 1.6553 - val_accuracy: 0.4228\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 1.4952 - accuracy: 0.4689 - val_loss: 1.5555 - val_accuracy: 0.4590\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 1.4231 - accuracy: 0.4961 - val_loss: 1.5517 - val_accuracy: 0.4664\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.3626 - accuracy: 0.5128 - val_loss: 1.5194 - val_accuracy: 0.4800\n",
            "Epoch 9/10\n",
            "352/352 [==============================] - 5s 13ms/step - loss: 1.3051 - accuracy: 0.5345 - val_loss: 1.5082 - val_accuracy: 0.4848\n",
            "Epoch 10/10\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 1.2645 - accuracy: 0.5478 - val_loss: 1.5112 - val_accuracy: 0.4828\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b3bb7ca55d0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the model can be evaluated on the validation data:"
      ],
      "metadata": {
        "id": "_l5v5YLtOYHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_valid_scaled, y_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YsJuflpOe9L",
        "outputId": "e06b93a7-39a0-46d9-96fa-d10a638b9dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 0s 3ms/step - loss: 1.5112 - accuracy: 0.4828\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5112253427505493, 0.4828000068664551]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since implementing advanced ML techniques such as alpha dropout regularization and 1cycle sdcheduling, my accuracy increased from 32% to 48%. I got to witness the importance of all of these small changes on the performance of the model over time, showcasing how ML techniques have improved model performance over time."
      ],
      "metadata": {
        "id": "s-3_W_O2Oow8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Works Cited (APA7)"
      ],
      "metadata": {
        "id": "UOz3Q_koSJxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brownlee, J. (2022, August 16). *Your first deep learning project in python with keras step-by-step.* MachineLearningMastery.com. https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
        "\n",
        "Géron, A. (2023). *Hands-on machine learning with scikit-learn, keras and tensorflow: Concepts, tools, and techniques to build Intelligent Systems* (3rd ed.). O’Reilly Media, Inc.\n",
        "\n",
        "Géron, A. (n.d.). Google Colab. *Chapter 11 – Training Deep Neural Networks*. https://colab.research.google.com/github/ageron/handson-ml3/blob/main/11_training_deep_neural_networks.ipynb\n",
        "\n",
        "Google. (n.d.). *Machine Learning Crash Course with TensorFlow APIs*. Google Machine Learning. https://developers.google.com/machine-learning/crash-course\n"
      ],
      "metadata": {
        "id": "gG0IUoQNTJMF"
      }
    }
  ]
}