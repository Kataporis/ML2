{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "4NA7B1IEp0PH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure that Python version 3.7 or above is installed:"
      ],
      "metadata": {
        "id": "VpQee9KWqtZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ],
      "metadata": {
        "id": "U5RExu4uq27y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import TensorFlow ≥ 2.8 * math:"
      ],
      "metadata": {
        "id": "Zszn9685wZJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ],
      "metadata": {
        "id": "X5en5voMwesD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customization"
      ],
      "metadata": {
        "id": "Fyi4MQns60Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The setup for this prject is identical to the previous one except for some small changes. Firstly, a custom activation function (leaky ReLU)  has been implemented instead of the previously used selu function. Second, a custom loss function (focal loss)  has been implemented instead of the previously used sparse categorical crossentropy function. I have kept my original notebook code to show all the changes that were made from this project 1 to this one."
      ],
      "metadata": {
        "id": "Ar7U8bk565d_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Deep Neural Network"
      ],
      "metadata": {
        "id": "HDNxVuhPzIHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I initially built the neural network with 5 hidden layers of 50 neurons each, though I ended up doubling the number of layers twice until I had 20. Furthermore, I employed he swish activation function and the he_normal kernel initializer. In order ensure the network self-normalizes, however, I standardized the input features, changed the activation function to selu, and switched to LeCun normal initialization.\n",
        "\n",
        "Now, for this project, I changed the activation function to a custom leaky ReLU function. This function is flexible and has been shown to work better for complex tasks such as this one. It is better able to capture nuance in data. When implementing this function, I set the initial alpha value was 0.01. Then 0.0001. Then 1. Then 1.25. Then 0.5. 0.5 seemed to yield the best results in terms of reducing loss, convergence, and accuracy."
      ],
      "metadata": {
        "id": "SndaAk7SFFA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define custom activation function Leaky ReLU\n",
        "def my_leaky_relu(z, alpha=1):\n",
        "    return tf.maximum(alpha * z, z)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(tf.keras.layers.Dense(50,\n",
        "                                    activation=my_leaky_relu,\n",
        "                                    kernel_initializer=\"lecun_normal\"))"
      ],
      "metadata": {
        "id": "Ow07_ID3_4AO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After some lackluster results, I decided to add some regularization with alpha dropout, as well."
      ],
      "metadata": {
        "id": "QSax1wt86uyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.AlphaDropout(rate = 0.1))"
      ],
      "metadata": {
        "id": "RKCymsXV680W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Network on the Dataset"
      ],
      "metadata": {
        "id": "69Kh8WFVXuD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the network was trained on the CIFAR10 dataset using Nadam optimization. I chose to train the network on the CIFAR10 dataset because it is one of the most common datasets for learning machine learning and is already built into Keras. Furthermore, the author of our course textbook Géron provided their approach to developing a model for this dataset in a Google Colab notebook. I frequently referenced this notebook and used it as a baseline for comparison."
      ],
      "metadata": {
        "id": "PwVN7JOyG8_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = tf.keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "LQbdSHPnhtwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b389d0b1-abb0-40d8-87da-3c7c116f50be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is composed of 60,000 32 x 32-pixel color images. 50,000 of these will be set aside for the training set, and the rest (10,000) will make up the testing set. There are 10 classes in this dataset, so the output layer should have 10 neurons with the softmax activation function. The softmax function scales numbers into probabilities. I added the output layer to the model as such:"
      ],
      "metadata": {
        "id": "0Y_P3_F3iKUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))"
      ],
      "metadata": {
        "id": "gg8z2qhbjGWG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I compiled the model and specified the stochastic gradient descent optimizer (I initially employed Nadam), sparse categorical crossentropy loss function, and accuracy metric to evaluate the the performance of the model. By compiling the model, \"the backend automatically chooses the best way to represent the network for training and making predictions to run on [the] hardware\" (Brownlee, 2022).\n",
        "\n",
        "\n",
        "Differently for this project, I chose to implement a custom focal loss function instead of the sparse categorical crossentropy function I had previously used. I hoped to improve my results by improving performance on difficult-to-classify examples in the data using this function. I kept the default values of alpha = 1 and gamma = 1, since they seemed to perform the best."
      ],
      "metadata": {
        "id": "_ANKrHNwnvh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom loss function (e.g., Focal Loss)\n",
        "class FocalLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, alpha=1, gamma=1):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
        "        focal_loss = -self.alpha * tf.pow(1.0 - y_pred, self.gamma) * y_true * tf.math.log(y_pred)\n",
        "        return tf.reduce_mean(focal_loss)\n",
        "\n",
        "model.compile(loss = FocalLoss(),\n",
        "              optimizer = tf.keras.optimizers.SGD(),\n",
        "              metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "Cca0CDb4o63_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The learning rate associated with the optimizer can be manually tuned by comparing the learning curves for different rates for a set number of epochs."
      ],
      "metadata": {
        "id": "FmNoDEndqH2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting the Model"
      ],
      "metadata": {
        "id": "nHfLSpz9rK6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to the train (fit) the model, it is first necessary to split the data into subsets. First, I split the data into only a training and testing subset, but I quickyl realized I would also need a validation set to best evaluate the model's performance. The validation set is composed of 5,000 images of the original training set."
      ],
      "metadata": {
        "id": "g9F3EvymuQ5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = cifar10\n",
        "\n",
        "X_train = X_train_full[5000:]\n",
        "y_train = y_train_full[5000:]\n",
        "X_valid = X_train_full[:5000]\n",
        "y_valid = y_train_full[:5000]\n",
        "\n",
        "X_means = X_train.mean(axis = 0)\n",
        "X_stds = X_train.std(axis = 0)\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled = (X_test - X_means) / X_stds"
      ],
      "metadata": {
        "id": "ctPpJ_Z8v8vQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to add 1cycle scheduling after hearing about how powerful it can be for this particular dataset. I utilized (Géron 2023)'s OneCycleScheduler custom callback class, find_learning_rate function, and ExponentialLearningRate callback class from the Chapter 11 Code for this purpose:"
      ],
      "metadata": {
        "id": "BQyxzpLxFEfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = tf.keras.backend\n",
        "# Updates the learning rate at the beginning of each batch. Increases learning rate linearly\n",
        "# during about the first half of training, then reduces it linearly back to initial\n",
        "# learning rate. Finally, it reduces it down to close to zero for the last part of\n",
        "# training.\n",
        "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, iterations, max_lr=1e-3, start_lr=None,\n",
        "                 last_iterations=None, last_lr=None):\n",
        "        self.iterations = iterations\n",
        "        self.max_lr = max_lr\n",
        "        self.start_lr = start_lr or max_lr / 10\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_lr = last_lr or self.start_lr / 1000\n",
        "        self.iteration = 0\n",
        "\n",
        "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
        "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
        "\n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            lr = self._interpolate(0, self.half_iteration, self.start_lr,\n",
        "                                   self.max_lr)\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
        "                                   self.max_lr, self.start_lr)\n",
        "        else:\n",
        "            lr = self._interpolate(2 * self.half_iteration, self.iterations,\n",
        "                                   self.start_lr, self.last_lr)\n",
        "        self.iteration += 1\n",
        "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
        "\n",
        "# Updates the learning rate during training at the end of each batch.\n",
        "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "        self.rates = []\n",
        "        self.losses = []\n",
        "\n",
        "# Trains the model using the ExponentialLearningRate callback and returns the\n",
        "# learning rates & batch losses.\n",
        "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=1e-4,\n",
        "                       max_rate=1):\n",
        "    init_weights = model.get_weights()\n",
        "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
        "    factor = (max_rate / min_rate) ** (1 / iterations)\n",
        "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
        "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
        "    exp_lr = ExponentialLearningRate(factor)\n",
        "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
        "                        callbacks=[exp_lr])\n",
        "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
        "    model.set_weights(init_weights)\n",
        "    return exp_lr.rates, exp_lr.losses"
      ],
      "metadata": {
        "id": "TOkqPpVtLsxc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "num_iterations = math.ceil(len(X_train_scaled) / batch_size) * num_epochs\n",
        "onecycle = OneCycleScheduler(num_iterations, max_lr = 0.05)\n",
        "# This finds the optimal max learning rate\n",
        "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1,\n",
        "                                   batch_size=batch_size)"
      ],
      "metadata": {
        "id": "qGeL9TalFj_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e88c9c71-d02d-4084-bd53-06fa92605ee7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "352/352 [==============================] - 4s 7ms/step - loss: 10.5836 - accuracy: 0.1025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the model can be trained on the training set over 10 epochs\n",
        ":"
      ],
      "metadata": {
        "id": "7Fc0ZHc5LtaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_scaled, y_train, epochs = num_epochs, batch_size = batch_size,\n",
        "          validation_data = (X_valid_scaled, y_valid),\n",
        "          callbacks = [onecycle])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UCTVq2ELxlL",
        "outputId": "dfcc6f30-9a03-4370-b691-c88f8e89fb6d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 10.5527 - accuracy: 0.0975 - val_loss: 9.4099 - val_accuracy: 0.0994\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 9.3695 - accuracy: 0.0983 - val_loss: 9.4009 - val_accuracy: 0.0972\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 3s 8ms/step - loss: 9.3703 - accuracy: 0.0993 - val_loss: 9.4473 - val_accuracy: 0.0992\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 4s 10ms/step - loss: 9.3777 - accuracy: 0.0993 - val_loss: 9.4393 - val_accuracy: 0.0970\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 9.3727 - accuracy: 0.0977 - val_loss: 9.4563 - val_accuracy: 0.0974\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 9.3386 - accuracy: 0.1024 - val_loss: 9.3838 - val_accuracy: 0.1008\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 3s 8ms/step - loss: 9.3201 - accuracy: 0.1009 - val_loss: 9.3795 - val_accuracy: 0.1242\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 9.3197 - accuracy: 0.1039 - val_loss: 9.3795 - val_accuracy: 0.1140\n",
            "Epoch 9/10\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 9.3196 - accuracy: 0.1056 - val_loss: 9.3795 - val_accuracy: 0.1174\n",
            "Epoch 10/10\n",
            "352/352 [==============================] - 3s 8ms/step - loss: 9.3196 - accuracy: 0.1053 - val_loss: 9.3795 - val_accuracy: 0.1164\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fdf5a261840>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the model can be evaluated on the validation data:"
      ],
      "metadata": {
        "id": "_l5v5YLtOYHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_valid_scaled, y_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YsJuflpOe9L",
        "outputId": "92f92f60-34fa-4ff4-ed8f-148c9c85a65a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 0s 2ms/step - loss: 9.3795 - accuracy: 0.1164\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9.379462242126465, 0.11640000343322754]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since implementing advanced ML techniques such as alpha dropout regularization and 1cycle sdcheduling, my accuracy increased from 32% to 48%. I got to witness the importance of all of these small changes on the performance of the model over time, showcasing how advanced ML techniques have improved model performance over time.\n",
        "\n",
        "NOW: After implementing the custom activation and loss functions, my performance actually decreased. My accuracy came in around 12% compared to the 48% I was previously getting. This was even after ample tuning of custom parameters. It just goes to show that sometimes it is best to stick to the simple, established methods. Nonetheless, it showed me how with enough knowledge and practice, I can fine-tune a complex model to achieve the best performance."
      ],
      "metadata": {
        "id": "s-3_W_O2Oow8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Works Cited (APA7)"
      ],
      "metadata": {
        "id": "UOz3Q_koSJxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brownlee, J. (2022, August 16). *Your first deep learning project in python with keras step-by-step.* MachineLearningMastery.com. https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
        "\n",
        "Géron, A. (2023). *Hands-on machine learning with scikit-learn, keras and tensorflow: Concepts, tools, and techniques to build Intelligent Systems* (3rd ed.). O’Reilly Media, Inc.\n",
        "\n",
        "Géron, A. (n.d.). Google Colab. *Chapter 11 – Training Deep Neural Networks*. https://colab.research.google.com/github/ageron/handson-ml3/blob/main/11_training_deep_neural_networks.ipynb\n",
        "\n",
        "Google. (n.d.). *Machine Learning Crash Course with TensorFlow APIs*. Google Machine Learning. https://developers.google.com/machine-learning/crash-course\n",
        "\n",
        "*Papers with code - leaky relu explained*. Explained | Papers With Code. (n.d.). https://paperswithcode.com/method/leaky-relu#:~:text=Leaky%20ReLU-,Edit,is%20not%20learnt%20during%20training.\n",
        "\n",
        "Trivedi, S. (2020, May 3). *Understanding focal loss - A quick read*. Medium. https://medium.com/visionwizard/understanding-focal-loss-a-quick-read-b914422913e7"
      ],
      "metadata": {
        "id": "gG0IUoQNTJMF"
      }
    }
  ]
}